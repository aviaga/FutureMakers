{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day 23 (7-28) - Project on NLP Creation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPC0tGLtixldy0SM1cbj4CB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aviaga/testing/blob/main/ML%20Secondary/Intro%20to%20NLPs%20-%20RoBERTa%20Model%20Creation%20(7-28).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NePu4gIgWNsC"
      },
      "source": [
        "# Project Intro\n",
        "This is the implementation of Day 23's Lesson Plan Item for SureStart's summer program, 2021.\n",
        "\n",
        "The goal of this Lesson Plan item was to experiment with NLPs. In this project, the goal was to build a RoBERTa model for the Spanish language. This model is able to predict the next words in a sentence given the appropriate context. \n",
        "\n",
        "\n",
        "The website where this code was adapted from can be found [here](https://colab.research.google.com/drive/1mXWYYkB9UjRdklPVSDvAcUDralmv3Pgv#scrollTo=LKs_0Gy998vO)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFAHK8r1Vlib"
      },
      "source": [
        "%%capture\n",
        "!pip uninstall -y tensorflow\n",
        "!pip install transformers==2.8.0\n",
        "\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7N8T966XLm7",
        "outputId": "c981aa06-a871-44b4-8b2a-b006a2afda03"
      },
      "source": [
        "import os\n",
        "# Importing dataset\n",
        "if not os.path.exists('data/dataset.txt'):\n",
        "  !wget \"https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2016/mono/es.txt.gz\" -O dataset.txt.gz\n",
        "  !gzip -d dataset.txt.gz\n",
        "  !mkdir data\n",
        "  !mv dataset.txt data"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-29 04:04:14--  https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2016/mono/es.txt.gz\n",
            "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
            "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1859673728 (1.7G) [application/gzip]\n",
            "Saving to: ‘dataset.txt.gz’\n",
            "\n",
            "dataset.txt.gz      100%[===================>]   1.73G  25.2MB/s    in 74s     \n",
            "\n",
            "2021-07-29 04:05:30 (23.8 MB/s) - ‘dataset.txt.gz’ saved [1859673728/1859673728]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX3jjJfYYVb5",
        "outputId": "e63c9ab3-043b-48ae-b128-c3d69495995d"
      },
      "source": [
        "#Looking at the data\n",
        "!wc -l data/dataset.txt\n",
        "!shuf -n 5 data/dataset.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "179287150 data/dataset.txt\n",
            "- Me refiero a la pistola.\n",
            "Mira las bromas que escribiste, mira ese chandal que me hiciste llevar.\n",
            "Estás borracho.\n",
            "¿Con tu paga?\n",
            "No salgo barato.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3BgOcsgYmHH"
      },
      "source": [
        "#Training and validation data\n",
        "TRAIN_SIZE = 1000000 #@param {type:\"integer\"}\n",
        "!(head -n $TRAIN_SIZE data/dataset.txt) > data/train.txt\n",
        "VAL_SIZE = 10000 #@param {type:\"integer\"}\n",
        "!(sed -n {TRAIN_SIZE + 1},{TRAIN_SIZE + VAL_SIZE}p data/dataset.txt) > data/dev.txt"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWlarGQAZJbw",
        "outputId": "b03a6447-984c-432f-c447-bc72be8a0d37"
      },
      "source": [
        "#Training tokenizer\n",
        "%%time\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "path = \"data/train.txt\"\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "tokenizer.train(files=path,\n",
        "                vocab_size=50265,\n",
        "                min_frequency=2,\n",
        "                special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])\n",
        "\n",
        "!mkdir -p \"models/roberta\"\n",
        "tokenizer.save(\"models/roberta\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 27.8 s, sys: 285 ms, total: 28.1 s\n",
            "Wall time: 28 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUHkIGUOZZTr"
      },
      "source": [
        "#Defining RoBERTa model architecture\n",
        "import json\n",
        "config = {\n",
        "\t\"architectures\": [\n",
        "\t\t\"RobertaForMaskedLM\"\n",
        "\t],\n",
        "\t\"attention_probs_dropout_prob\": 0.1,\n",
        "\t\"hidden_act\": \"gelu\",\n",
        "\t\"hidden_dropout_prob\": 0.1,\n",
        "\t\"hidden_size\": 768,\n",
        "\t\"initializer_range\": 0.02,\n",
        "\t\"intermediate_size\": 3072,\n",
        "\t\"layer_norm_eps\": 1e-05,\n",
        "\t\"max_position_embeddings\": 514,\n",
        "\t\"model_type\": \"roberta\",\n",
        "\t\"num_attention_heads\": 12,\n",
        "\t\"num_hidden_layers\": 12,\n",
        "\t\"type_vocab_size\": 1,\n",
        "\t\"vocab_size\": 50265\n",
        "}\n",
        "\n",
        "with open(\"models/roberta/config.json\", 'w') as fp:\n",
        "    json.dump(config, fp)\n",
        "\n",
        "tokenizer_config = {\"max_len\": 512}\n",
        "\n",
        "with open(\"models/roberta/tokenizer_config.json\", 'w') as fp:\n",
        "    json.dump(tokenizer_config, fp)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRpTt1iTZxQ0",
        "outputId": "5b2e38cb-363c-47bd-adc0-401732d057e7"
      },
      "source": [
        "#Model training and defining paths\n",
        "!wget -c https://raw.githubusercontent.com/chriskhanhtran/spanish-bert/master/run_language_modeling.py\n",
        "MODEL_TYPE = \"roberta\" #@param [\"roberta\", \"bert\"]\n",
        "MODEL_DIR = \"models/roberta\" #@param {type: \"string\"}\n",
        "OUTPUT_DIR = \"models/roberta/output\" #@param {type: \"string\"}\n",
        "TRAIN_PATH = \"data/train.txt\" #@param {type: \"string\"}\n",
        "EVAL_PATH = \"data/dev.txt\" #@param {type: \"string\"}\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-29 04:09:00--  https://raw.githubusercontent.com/chriskhanhtran/spanish-bert/master/run_language_modeling.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 34328 (34K) [text/plain]\n",
            "Saving to: ‘run_language_modeling.py’\n",
            "\n",
            "run_language_modeli 100%[===================>]  33.52K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-07-29 04:09:01 (107 MB/s) - ‘run_language_modeling.py’ saved [34328/34328]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A0xAWSVaMAQ"
      },
      "source": [
        "#Continue model training\n",
        "train_params = {\n",
        "    \"output_dir\": OUTPUT_DIR,\n",
        "    \"model_type\": MODEL_TYPE,\n",
        "    \"config_name\": MODEL_DIR,\n",
        "    \"tokenizer_name\": MODEL_DIR,\n",
        "    \"train_path\": TRAIN_PATH,\n",
        "    \"eval_path\": EVAL_PATH,\n",
        "    \"do_eval\": \"--do_eval\",\n",
        "    \"evaluate_during_training\": \"\",\n",
        "    \"line_by_line\": \"\",\n",
        "    \"should_continue\": \"\",\n",
        "    \"model_name_or_path\": \"\",\n",
        "}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2a1tL6SaRV6",
        "outputId": "b3edb417-d0c1-426e-a29f-97e66fe6af03"
      },
      "source": [
        "pip install tensorboard==2.1.0"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboard==2.1.0\n",
            "  Downloading tensorboard-2.1.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 30.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.1.0) (1.15.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.1.0) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.1.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.1.0) (1.32.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.1.0) (1.19.5)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.1.0) (0.12.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.1.0) (3.3.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.1.0) (0.36.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.1.0) (1.34.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.1.0) (57.2.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.1.0) (3.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.1.0) (0.4.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.1.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.1.0) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.1.0) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.1.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard==2.1.0) (4.6.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard==2.1.0) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard==2.1.0) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard==2.1.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard==2.1.0) (1.25.11)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.1.0) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard==2.1.0) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard==2.1.0) (3.7.4.3)\n",
            "Installing collected packages: tensorboard\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.5.0\n",
            "    Uninstalling tensorboard-2.5.0:\n",
            "      Successfully uninstalled tensorboard-2.5.0\n",
            "Successfully installed tensorboard-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R26ycLJBbHmB"
      },
      "source": [
        "cmd = \"\"\"python run_language_modeling.py \\\n",
        "    --output_dir {output_dir} \\\n",
        "    --model_type {model_type} \\\n",
        "    --mlm \\\n",
        "    --config_name {config_name} \\\n",
        "    --tokenizer_name {tokenizer_name} \\\n",
        "    {line_by_line} \\\n",
        "    {should_continue} \\\n",
        "    {model_name_or_path} \\\n",
        "    --train_data_file {train_path} \\\n",
        "    --eval_data_file {eval_path} \\\n",
        "    --do_train \\\n",
        "    {do_eval} \\\n",
        "    {evaluate_during_training} \\\n",
        "    --overwrite_output_dir \\\n",
        "    --block_size 512 \\\n",
        "    --max_step 25 \\\n",
        "    --warmup_steps 10 \\\n",
        "    --learning_rate 5e-5 \\\n",
        "    --per_gpu_train_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 4 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --adam_epsilon 1e-6 \\\n",
        "    --max_grad_norm 100.0 \\\n",
        "    --save_total_limit 10 \\\n",
        "    --save_steps 10 \\\n",
        "    --logging_steps 2 \\\n",
        "    --seed 42\n",
        "\"\"\""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9mPJSTRaY8y",
        "outputId": "fa03fb4a-e99d-4aef-f7b4-b9d3a35937bb"
      },
      "source": [
        "!{cmd.format(**train_params)}"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "07/29/2021 04:09:12 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "07/29/2021 04:09:12 - INFO - transformers.configuration_utils -   loading configuration file models/roberta/config.json\n",
            "07/29/2021 04:09:12 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "07/29/2021 04:09:12 - INFO - transformers.configuration_utils -   loading configuration file models/roberta/config.json\n",
            "07/29/2021 04:09:12 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "07/29/2021 04:09:12 - INFO - transformers.tokenization_utils -   Model name 'models/roberta' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'models/roberta' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "07/29/2021 04:09:12 - INFO - transformers.tokenization_utils -   Didn't find file models/roberta/added_tokens.json. We won't load it.\n",
            "07/29/2021 04:09:12 - INFO - transformers.tokenization_utils -   Didn't find file models/roberta/special_tokens_map.json. We won't load it.\n",
            "07/29/2021 04:09:12 - INFO - transformers.tokenization_utils -   loading file models/roberta/vocab.json\n",
            "07/29/2021 04:09:12 - INFO - transformers.tokenization_utils -   loading file models/roberta/merges.txt\n",
            "07/29/2021 04:09:12 - INFO - transformers.tokenization_utils -   loading file None\n",
            "07/29/2021 04:09:12 - INFO - transformers.tokenization_utils -   loading file None\n",
            "07/29/2021 04:09:12 - INFO - transformers.tokenization_utils -   loading file models/roberta/tokenizer_config.json\n",
            "07/29/2021 04:09:12 - INFO - __main__ -   Training new model from scratch\n",
            "07/29/2021 04:09:29 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-06, block_size=512, cache_dir=None, config_name='models/roberta', device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='data/dev.txt', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=4, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=2, max_grad_norm=100.0, max_steps=25, mlm=True, mlm_probability=0.15, model_name_or_path=None, model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='models/roberta/output', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=10, save_total_limit=10, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name='models/roberta', train_data_file='data/train.txt', warmup_steps=10, weight_decay=0.01)\n",
            "07/29/2021 04:09:29 - INFO - __main__ -   Creating features from dataset file at data\n",
            "07/29/2021 04:10:05 - INFO - __main__ -   Saving features into cached file data/roberta_cached_lm_510_train.txt\n",
            "07/29/2021 04:10:06 - INFO - __main__ -   ***** Running training *****\n",
            "07/29/2021 04:10:06 - INFO - __main__ -     Num examples = 15932\n",
            "07/29/2021 04:10:06 - INFO - __main__ -     Num Epochs = 1\n",
            "07/29/2021 04:10:06 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
            "07/29/2021 04:10:06 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "07/29/2021 04:10:06 - INFO - __main__ -     Gradient Accumulation steps = 4\n",
            "07/29/2021 04:10:06 - INFO - __main__ -     Total optimization steps = 25\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/3983 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/3983 [00:00<58:11,  1.14it/s]\u001b[A\n",
            "Iteration:   0% 2/3983 [00:01<50:53,  1.30it/s]\u001b[A\n",
            "Iteration:   0% 3/3983 [00:01<46:10,  1.44it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "\n",
            "Iteration:   0% 4/3983 [00:02<43:56,  1.51it/s]\u001b[A\n",
            "Iteration:   0% 5/3983 [00:03<40:55,  1.62it/s]\u001b[A\n",
            "Iteration:   0% 6/3983 [00:03<39:13,  1.69it/s]\u001b[A\n",
            "Iteration:   0% 7/3983 [00:04<37:30,  1.77it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "\n",
            "Iteration:   0% 8/3983 [00:04<37:45,  1.75it/s]\u001b[A\n",
            "Iteration:   0% 9/3983 [00:05<36:35,  1.81it/s]\u001b[A\n",
            "Iteration:   0% 10/3983 [00:05<36:14,  1.83it/s]\u001b[A\n",
            "Iteration:   0% 11/3983 [00:06<35:42,  1.85it/s]\u001b[A\n",
            "Iteration:   0% 12/3983 [00:06<36:27,  1.82it/s]\u001b[A\n",
            "Iteration:   0% 13/3983 [00:07<35:46,  1.85it/s]\u001b[A\n",
            "Iteration:   0% 14/3983 [00:07<35:38,  1.86it/s]\u001b[A\n",
            "Iteration:   0% 15/3983 [00:08<35:19,  1.87it/s]\u001b[A\n",
            "Iteration:   0% 16/3983 [00:08<36:11,  1.83it/s]\u001b[A\n",
            "Iteration:   0% 17/3983 [00:09<35:38,  1.85it/s]\u001b[A\n",
            "Iteration:   0% 18/3983 [00:09<35:28,  1.86it/s]\u001b[A\n",
            "Iteration:   0% 19/3983 [00:10<35:12,  1.88it/s]\u001b[A\n",
            "Iteration:   1% 20/3983 [00:11<36:07,  1.83it/s]\u001b[A\n",
            "Iteration:   1% 21/3983 [00:11<35:56,  1.84it/s]\u001b[A\n",
            "Iteration:   1% 22/3983 [00:12<35:44,  1.85it/s]\u001b[A\n",
            "Iteration:   1% 23/3983 [00:12<35:24,  1.86it/s]\u001b[A\n",
            "Iteration:   1% 24/3983 [00:13<36:29,  1.81it/s]\u001b[A\n",
            "Iteration:   1% 25/3983 [00:13<35:51,  1.84it/s]\u001b[A\n",
            "Iteration:   1% 26/3983 [00:14<35:54,  1.84it/s]\u001b[A\n",
            "Iteration:   1% 27/3983 [00:14<35:35,  1.85it/s]\u001b[A\n",
            "Iteration:   1% 28/3983 [00:15<36:20,  1.81it/s]\u001b[A\n",
            "Iteration:   1% 29/3983 [00:15<35:59,  1.83it/s]\u001b[A\n",
            "Iteration:   1% 30/3983 [00:16<35:52,  1.84it/s]\u001b[A\n",
            "Iteration:   1% 31/3983 [00:17<35:39,  1.85it/s]\u001b[A\n",
            "Iteration:   1% 32/3983 [00:17<36:42,  1.79it/s]\u001b[A\n",
            "Iteration:   1% 33/3983 [00:18<36:06,  1.82it/s]\u001b[A\n",
            "Iteration:   1% 34/3983 [00:18<36:03,  1.83it/s]\u001b[A\n",
            "Iteration:   1% 35/3983 [00:19<35:41,  1.84it/s]\u001b[A\n",
            "Iteration:   1% 36/3983 [00:19<36:37,  1.80it/s]\u001b[A\n",
            "Iteration:   1% 37/3983 [00:20<36:01,  1.83it/s]\u001b[A\n",
            "Iteration:   1% 38/3983 [00:20<35:57,  1.83it/s]\u001b[A\n",
            "Iteration:   1% 39/3983 [00:21<35:56,  1.83it/s]\u001b[A07/29/2021 04:10:28 - INFO - transformers.configuration_utils -   Configuration saved in models/roberta/output/checkpoint-10/config.json\n",
            "07/29/2021 04:10:29 - INFO - transformers.modeling_utils -   Model weights saved in models/roberta/output/checkpoint-10/pytorch_model.bin\n",
            "07/29/2021 04:10:30 - INFO - __main__ -   Saving model checkpoint to models/roberta/output/checkpoint-10\n",
            "07/29/2021 04:10:34 - INFO - __main__ -   Saving optimizer and scheduler states to models/roberta/output/checkpoint-10\n",
            "\n",
            "Iteration:   1% 40/3983 [00:28<2:36:19,  2.38s/it]\u001b[A\n",
            "Iteration:   1% 41/3983 [00:28<2:00:43,  1.84s/it]\u001b[A\n",
            "Iteration:   1% 42/3983 [00:29<1:37:24,  1.48s/it]\u001b[A\n",
            "Iteration:   1% 43/3983 [00:29<1:19:15,  1.21s/it]\u001b[A\n",
            "Iteration:   1% 44/3983 [00:30<1:06:48,  1.02s/it]\u001b[A\n",
            "Iteration:   1% 45/3983 [00:31<57:34,  1.14it/s]  \u001b[A\n",
            "Iteration:   1% 46/3983 [00:31<50:43,  1.29it/s]\u001b[A\n",
            "Iteration:   1% 47/3983 [00:32<46:16,  1.42it/s]\u001b[A\n",
            "Iteration:   1% 48/3983 [00:32<44:00,  1.49it/s]\u001b[A\n",
            "Iteration:   1% 49/3983 [00:33<41:32,  1.58it/s]\u001b[A\n",
            "Iteration:   1% 50/3983 [00:33<39:51,  1.64it/s]\u001b[A\n",
            "Iteration:   1% 51/3983 [00:34<38:41,  1.69it/s]\u001b[A\n",
            "Iteration:   1% 52/3983 [00:34<38:49,  1.69it/s]\u001b[A\n",
            "Iteration:   1% 53/3983 [00:35<37:51,  1.73it/s]\u001b[A\n",
            "Iteration:   1% 54/3983 [00:36<37:17,  1.76it/s]\u001b[A\n",
            "Iteration:   1% 55/3983 [00:36<36:53,  1.77it/s]\u001b[A\n",
            "Iteration:   1% 56/3983 [00:37<37:31,  1.74it/s]\u001b[A\n",
            "Iteration:   1% 57/3983 [00:37<36:55,  1.77it/s]\u001b[A\n",
            "Iteration:   1% 58/3983 [00:38<36:38,  1.78it/s]\u001b[A\n",
            "Iteration:   1% 59/3983 [00:38<36:23,  1.80it/s]\u001b[A\n",
            "Iteration:   2% 60/3983 [00:39<37:06,  1.76it/s]\u001b[A\n",
            "Iteration:   2% 61/3983 [00:39<36:39,  1.78it/s]\u001b[A\n",
            "Iteration:   2% 62/3983 [00:40<36:23,  1.80it/s]\u001b[A\n",
            "Iteration:   2% 63/3983 [00:41<36:20,  1.80it/s]\u001b[A\n",
            "Iteration:   2% 64/3983 [00:41<37:17,  1.75it/s]\u001b[A\n",
            "Iteration:   2% 65/3983 [00:42<36:55,  1.77it/s]\u001b[A\n",
            "Iteration:   2% 66/3983 [00:42<36:27,  1.79it/s]\u001b[A\n",
            "Iteration:   2% 67/3983 [00:43<36:33,  1.78it/s]\u001b[A\n",
            "Iteration:   2% 68/3983 [00:43<37:13,  1.75it/s]\u001b[A\n",
            "Iteration:   2% 69/3983 [00:44<36:43,  1.78it/s]\u001b[A\n",
            "Iteration:   2% 70/3983 [00:45<36:26,  1.79it/s]\u001b[A\n",
            "Iteration:   2% 71/3983 [00:45<36:14,  1.80it/s]\u001b[A\n",
            "Iteration:   2% 72/3983 [00:46<37:08,  1.76it/s]\u001b[A\n",
            "Iteration:   2% 73/3983 [00:46<36:48,  1.77it/s]\u001b[A\n",
            "Iteration:   2% 74/3983 [00:47<36:28,  1.79it/s]\u001b[A\n",
            "Iteration:   2% 75/3983 [00:47<36:16,  1.80it/s]\u001b[A\n",
            "Iteration:   2% 76/3983 [00:48<37:14,  1.75it/s]\u001b[A\n",
            "Iteration:   2% 77/3983 [00:48<36:46,  1.77it/s]\u001b[A\n",
            "Iteration:   2% 78/3983 [00:49<36:27,  1.78it/s]\u001b[A\n",
            "Iteration:   2% 79/3983 [00:50<36:20,  1.79it/s]\u001b[A07/29/2021 04:10:56 - INFO - transformers.configuration_utils -   Configuration saved in models/roberta/output/checkpoint-20/config.json\n",
            "07/29/2021 04:10:58 - INFO - transformers.modeling_utils -   Model weights saved in models/roberta/output/checkpoint-20/pytorch_model.bin\n",
            "07/29/2021 04:10:58 - INFO - __main__ -   Saving model checkpoint to models/roberta/output/checkpoint-20\n",
            "07/29/2021 04:11:12 - INFO - __main__ -   Saving optimizer and scheduler states to models/roberta/output/checkpoint-20\n",
            "\n",
            "Iteration:   2% 80/3983 [01:06<5:46:59,  5.33s/it]\u001b[A\n",
            "Iteration:   2% 81/3983 [01:07<4:14:24,  3.91s/it]\u001b[A\n",
            "Iteration:   2% 82/3983 [01:07<3:08:54,  2.91s/it]\u001b[A\n",
            "Iteration:   2% 83/3983 [01:08<2:23:02,  2.20s/it]\u001b[A\n",
            "Iteration:   2% 84/3983 [01:08<1:51:59,  1.72s/it]\u001b[A\n",
            "Iteration:   2% 85/3983 [01:09<1:29:03,  1.37s/it]\u001b[A\n",
            "Iteration:   2% 86/3983 [01:09<1:13:20,  1.13s/it]\u001b[A\n",
            "Iteration:   2% 87/3983 [01:10<1:02:06,  1.05it/s]\u001b[A\n",
            "Iteration:   2% 88/3983 [01:11<55:27,  1.17it/s]  \u001b[A\n",
            "Iteration:   2% 89/3983 [01:11<49:44,  1.30it/s]\u001b[A\n",
            "Iteration:   2% 90/3983 [01:12<45:45,  1.42it/s]\u001b[A\n",
            "Iteration:   2% 91/3983 [01:12<43:08,  1.50it/s]\u001b[A\n",
            "Iteration:   2% 92/3983 [01:13<42:11,  1.54it/s]\u001b[A\n",
            "Iteration:   2% 93/3983 [01:14<40:35,  1.60it/s]\u001b[A\n",
            "Iteration:   2% 94/3983 [01:14<39:21,  1.65it/s]\u001b[A\n",
            "Iteration:   2% 95/3983 [01:15<38:39,  1.68it/s]\u001b[A\n",
            "Iteration:   2% 96/3983 [01:15<38:58,  1.66it/s]\u001b[A\n",
            "Iteration:   2% 97/3983 [01:16<38:14,  1.69it/s]\u001b[A\n",
            "Iteration:   2% 98/3983 [01:16<37:50,  1.71it/s]\u001b[A\n",
            "Iteration:   2% 99/3983 [01:17<37:28,  1.73it/s]\u001b[A\n",
            "Iteration:   3% 100/3983 [01:18<38:17,  1.69it/s]\u001b[A\n",
            "Iteration:   3% 101/3983 [01:18<37:55,  1.71it/s]\u001b[A\n",
            "Iteration:   3% 102/3983 [01:19<37:49,  1.71it/s]\u001b[A\n",
            "Iteration:   3% 103/3983 [01:20<50:31,  1.28it/s]\n",
            "Epoch:   0% 0/1 [01:20<?, ?it/s]\n",
            "07/29/2021 04:11:26 - INFO - __main__ -    global_step = 26, average loss = 9.33139206812932\n",
            "07/29/2021 04:11:26 - INFO - __main__ -   Saving model checkpoint to models/roberta/output\n",
            "07/29/2021 04:11:26 - INFO - transformers.configuration_utils -   Configuration saved in models/roberta/output/config.json\n",
            "07/29/2021 04:11:28 - INFO - transformers.modeling_utils -   Model weights saved in models/roberta/output/pytorch_model.bin\n",
            "07/29/2021 04:11:28 - INFO - transformers.configuration_utils -   loading configuration file models/roberta/output/config.json\n",
            "07/29/2021 04:11:28 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "07/29/2021 04:11:28 - INFO - transformers.modeling_utils -   loading weights file models/roberta/output/pytorch_model.bin\n",
            "07/29/2021 04:11:33 - INFO - transformers.configuration_utils -   loading configuration file models/roberta/output/config.json\n",
            "07/29/2021 04:11:33 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "07/29/2021 04:11:33 - INFO - transformers.tokenization_utils -   Model name 'models/roberta/output' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'models/roberta/output' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "07/29/2021 04:11:33 - INFO - transformers.tokenization_utils -   Didn't find file models/roberta/output/added_tokens.json. We won't load it.\n",
            "07/29/2021 04:11:33 - INFO - transformers.tokenization_utils -   loading file models/roberta/output/vocab.json\n",
            "07/29/2021 04:11:33 - INFO - transformers.tokenization_utils -   loading file models/roberta/output/merges.txt\n",
            "07/29/2021 04:11:33 - INFO - transformers.tokenization_utils -   loading file None\n",
            "07/29/2021 04:11:33 - INFO - transformers.tokenization_utils -   loading file models/roberta/output/special_tokens_map.json\n",
            "07/29/2021 04:11:33 - INFO - transformers.tokenization_utils -   loading file models/roberta/output/tokenizer_config.json\n",
            "07/29/2021 04:11:33 - INFO - __main__ -   Evaluate the following checkpoints: ['models/roberta/output']\n",
            "07/29/2021 04:11:33 - INFO - transformers.configuration_utils -   loading configuration file models/roberta/output/config.json\n",
            "07/29/2021 04:11:33 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "07/29/2021 04:11:33 - INFO - transformers.modeling_utils -   loading weights file models/roberta/output/pytorch_model.bin\n",
            "07/29/2021 04:11:39 - INFO - __main__ -   Creating features from dataset file at data\n",
            "07/29/2021 04:11:39 - INFO - __main__ -   Saving features into cached file data/roberta_cached_lm_510_dev.txt\n",
            "07/29/2021 04:11:39 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "07/29/2021 04:11:39 - INFO - __main__ -     Num examples = 180\n",
            "07/29/2021 04:11:39 - INFO - __main__ -     Batch size = 4\n",
            "Evaluating: 100% 45/45 [00:09<00:00,  4.51it/s]\n",
            "07/29/2021 04:11:49 - INFO - __main__ -   ***** Eval results  *****\n",
            "07/29/2021 04:11:49 - INFO - __main__ -     perplexity = tensor(7340.6953)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMYHLvp_dr0J"
      },
      "source": [
        "#Making a prediction - setting it up\n",
        "%%capture\n",
        "%%time\n",
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline( \"fill-mask\", model=\"chriskhanhtran/spanberta\",tokenizer=\"chriskhanhtran/spanberta\")\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlFfYOqAd6C3",
        "outputId": "460f8abc-6c84-4c05-9a77-367e3a085060"
      },
      "source": [
        "#The prediction. In English, the input states \"one should wash their hands frequently with soap and ____\". The model then predicts possible options, with them being soap, salt, steam, lemon, and vinegar, respectively\n",
        "fill_mask(\"Lavarse frecuentemente las manos con agua y <mask>.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.6469604969024658,\n",
              "  'sequence': '<s> Lavarse frecuentemente las manos con agua y jabón.</s>',\n",
              "  'token': 18493},\n",
              " {'score': 0.06074365973472595,\n",
              "  'sequence': '<s> Lavarse frecuentemente las manos con agua y sal.</s>',\n",
              "  'token': 619},\n",
              " {'score': 0.029788149520754814,\n",
              "  'sequence': '<s> Lavarse frecuentemente las manos con agua y vapor.</s>',\n",
              "  'token': 11079},\n",
              " {'score': 0.0264101754873991,\n",
              "  'sequence': '<s> Lavarse frecuentemente las manos con agua y limón.</s>',\n",
              "  'token': 12788},\n",
              " {'score': 0.01702934503555298,\n",
              "  'sequence': '<s> Lavarse frecuentemente las manos con agua y vinagre.</s>',\n",
              "  'token': 18424}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    }
  ]
}
